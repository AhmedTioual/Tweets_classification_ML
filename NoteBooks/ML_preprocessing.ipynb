{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fq1vKXZn-ME"
      },
      "source": [
        "# Loading Data Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nmH7h_XtT0j",
        "outputId": "e05e120b-26c3-4741-8fa7-905187ab6f8a"
      },
      "outputs": [],
      "source": [
        "!pip install pandas nltk spacy langdetect tashaphyne pymongo demoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oWG8Vu6vAox",
        "outputId": "f4a1e532-efe5-4618-a452-86257bb80155"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwh9Z90coAlJ"
      },
      "outputs": [],
      "source": [
        "from pymongo import MongoClient\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import demoji\n",
        "import string\n",
        "from tashaphyne.stemming import ArabicLightStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw-9AkdfuYUv",
        "outputId": "fa11567d-1716-43bf-dbde-f2c3df50a017"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "demoji.download_codes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyA6sJzPuS5b",
        "outputId": "692426ff-f592-4e92-e2d8-e6010176efc3"
      },
      "outputs": [],
      "source": [
        "stemmer_en = PorterStemmer()\n",
        "stemmer_fr = SnowballStemmer('french')\n",
        "stemmer_ar = ISRIStemmer()\n",
        "lemmatizer_en = WordNetLemmatizer()\n",
        "ar_stemmer = ArabicLightStemmer()\n",
        "nlp_en = spacy.load('en_core_web_sm')\n",
        "nlp_fr = spacy.load('fr_core_news_sm')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gybesKgpwwC"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iygbjyb7oFFK"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    # Settings\n",
        "    username = \"mlteam\"\n",
        "    password = \"mlteam\"\n",
        "    database_name = \"TweetsDataBase\"\n",
        "    collection_name = \"TweetsData\"\n",
        "    uri = f\"mongodb+srv://{username}:{password}@cluster0.6y3bpz0.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
        "\n",
        "    client = MongoClient(uri)\n",
        "    db = client[database_name]\n",
        "    collection = db[collection_name]\n",
        "    documents = list(collection.find({}, {'full_text': 1, 'lang':1,'topic': 1, '_id': 0}))\n",
        "\n",
        "    df = pd.DataFrame(documents)\n",
        "    df.rename(columns={'full_text': 'tweet'}, inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qra6BEH6otis"
      },
      "source": [
        "#Preprocessing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlBDfOtwo0y3"
      },
      "source": [
        "## Sentence Segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs1pMhOY13bm",
        "outputId": "fefdde3c-75e0-4315-f602-6e8194cb52e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2024.2.2)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n"
          ]
        }
      ],
      "source": [
        "!pip install farasapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_m_4Oy611uo"
      },
      "outputs": [],
      "source": [
        "#from farasa.segmenter import FarasaSegmenter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7bSuulT2RhK"
      },
      "outputs": [],
      "source": [
        "# Initialize Farasa Segmenter (only if you are using it)\n",
        "#farasa_segmenter = FarasaSegmenter(interactive=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sTelkFgm7Il"
      },
      "outputs": [],
      "source": [
        "def sentence_segmentation(text, lang):\n",
        "    if lang == 'en':\n",
        "        return nltk.sent_tokenize(text)\n",
        "    elif lang == 'fr':\n",
        "        return nltk.sent_tokenize(text)\n",
        "    elif lang == 'ar':\n",
        "        # Using Farasa for Arabic sentence segmentation if available\n",
        "        return farasa_segmenter.segment(text)\n",
        "    else:\n",
        "        # Fallback to English sentence tokenization\n",
        "        return nltk.sent_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KinSC5Y4o-7l"
      },
      "source": [
        "## Dependency Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA4Blq_NnTiV"
      },
      "outputs": [],
      "source": [
        "def dependency_parsing(tokens, lang):\n",
        "    if lang == 'en':\n",
        "        text = ' '.join(tokens)\n",
        "        doc = nlp_en(text)\n",
        "        return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "    elif lang == 'fr':\n",
        "        text = ' '.join(tokens)\n",
        "        doc = nlp_fr(text)\n",
        "        return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "    elif lang == 'ar':\n",
        "        return []  # Dependency parsing for Arabic not supported by spaCy\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPBWl9BApBJK"
      },
      "source": [
        "## Part-of-Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZIf0docnabc"
      },
      "outputs": [],
      "source": [
        "def pos_tagging(tokens, lang):\n",
        "    if lang == 'en':\n",
        "        return nltk.pos_tag(tokens)\n",
        "    elif lang == 'fr':\n",
        "        return [(token.text, token.pos_) for token in nlp_fr(' '.join(tokens))]\n",
        "    elif lang == 'ar':\n",
        "        return [(token, 'N/A') for token in tokens]  # Custom POS tagging for Arabic\n",
        "    return []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Crvz6yp8loc4"
      },
      "source": [
        "## Data Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSQKb1zalsDb"
      },
      "outputs": [],
      "source": [
        "def data_quality(data_frame):\n",
        "  data_frame['tweet'] = data_frame['tweet'].str.lower()\n",
        "\n",
        "  filter_condition_eco = (\n",
        "      (data_frame['topic'] == 'Economy') &\n",
        "      (\n",
        "          data_frame['tweet'].str.contains('eco') |\n",
        "          data_frame['tweet'].str.contains('invest') |\n",
        "          data_frame['tweet'].str.contains('قتص') |\n",
        "          data_frame['tweet'].str.contains('مال') |\n",
        "          data_frame['tweet'].str.contains('تجار')\n",
        "      )\n",
        "  )\n",
        "  # Update the original DataFrame to keep only the filtered data for 'Economy' topic\n",
        "  data_frame = data_frame.loc[~(data_frame['topic'] == 'Economy') | filter_condition_eco]\n",
        "\n",
        "  filter_condition_politics = (\n",
        "      (data_frame['topic'] == 'Politics') &\n",
        "      (\n",
        "          data_frame['tweet'].str.contains('polit') |\n",
        "          data_frame['tweet'].str.contains('سياس') |\n",
        "          data_frame['tweet'].str.contains('حكو')\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Update the original DataFrame to keep only the filtered data for 'Economy' topic\n",
        "  data_frame = data_frame.loc[~(data_frame['topic'] == 'Politics') | filter_condition_politics]\n",
        "\n",
        "  filter_condition_tourism = (\n",
        "      (data_frame['topic'] == 'Tourism') &\n",
        "      (\n",
        "          data_frame['tweet'].str.contains('touri') |\n",
        "          data_frame['tweet'].str.contains('سياح')\n",
        "      )\n",
        "  )\n",
        "  # Update the original DataFrame to keep only the filtered data for 'Economy' topic\n",
        "  data_frame = data_frame.loc[~(data_frame['topic'] == 'Tourism') | filter_condition_tourism]\n",
        "\n",
        "  filter_condition_techno = (\n",
        "      (data_frame['topic'] == 'Technology') &\n",
        "      (\n",
        "          data_frame['tweet'].str.contains('techno') |\n",
        "          data_frame['tweet'].str.contains('تكنو')\n",
        "      )\n",
        "  )\n",
        "\n",
        "  # Update the original DataFrame to keep only the filtered data for '' topic\n",
        "  data_frame = data_frame.loc[~(data_frame['topic'] == 'Technology') | filter_condition_techno]\n",
        "\n",
        "  return data_frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toPwlvoZoylr"
      },
      "source": [
        "## Cleaning Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_a69jCUm5HN"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    #remove hyperlinks\n",
        "    tweet = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove tags (@xxxxx)\n",
        "    tweet = re.sub(r'@\\w+', '', tweet)\n",
        "    # Remove special characters\n",
        "    tweet = re.sub(r'\\W', ' ', tweet)\n",
        "    # Remove emojis\n",
        "    tweet = demoji.replace(tweet, '')\n",
        "    # Remove punctuation\n",
        "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove digits\n",
        "    tweet = re.sub(r'[\\W\\d٠١٢٣٤٥٦٧٨٩]', ' ', tweet)\n",
        "    # Lowercase\n",
        "    return tweet.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3dXi13Fo3Iy"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EQFMQg6m9e8"
      },
      "outputs": [],
      "source": [
        "def word_tokenization(text, lang):\n",
        "    if lang == 'en':\n",
        "        return word_tokenize(text)\n",
        "    elif lang == 'fr':\n",
        "        return word_tokenize(text)\n",
        "    elif lang == 'ar':\n",
        "        #arabic_tokens = re.findall(r'\\b[\\w\\']+\\b', text, re.UNICODE)\n",
        "        return word_tokenize(text)#arabic_tokens\n",
        "    else:\n",
        "        # Fallback to English\n",
        "        return word_tokenize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be3HVA07o5Ia"
      },
      "source": [
        "## Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5TpFdAQ4ubq"
      },
      "outputs": [],
      "source": [
        "def stemming(tokens, lang):\n",
        "    if lang == 'en':\n",
        "        return [stemmer_en.stem(token) for token in tokens]\n",
        "    elif lang == 'fr':\n",
        "        return [stemmer_fr.stem(token) for token in tokens]\n",
        "    elif lang == 'ar':\n",
        "        return [stemmer_ar.stem(token) for token in tokens]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9nPaMa4m-9D"
      },
      "outputs": [],
      "source": [
        "def lemmatization(tokens, lang):\n",
        "    if lang == 'en':\n",
        "        tokens = word_tokenize(tokens)\n",
        "        return [lemmatizer_en.lemmatize(word) for word in tokens]\n",
        "    elif lang == 'fr':\n",
        "        doc = nlp_fr(tokens)\n",
        "        return [token.lemma_ for token in doc if token.is_alpha]\n",
        "    elif lang == 'ar':\n",
        "        tokens = word_tokenize(tokens)\n",
        "        return [ar_stemmer.light_stem(word) for word in tokens]\n",
        "    return [lemmatizer_en.lemmatize(token) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwsOsmC4o8lc"
      },
      "source": [
        "## Stop Word Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUZUbS9wuDDO"
      },
      "outputs": [],
      "source": [
        "stop_words_en = set(stopwords.words('english'))\n",
        "stop_words_fr = set(stopwords.words('french'))\n",
        "stop_words_ar = set(stopwords.words('arabic'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOslzPtFnBHR"
      },
      "outputs": [],
      "source": [
        "def remove_stop_words(tokens, lang):\n",
        "    if lang == 'en':\n",
        "        return [token for token in tokens if token.lower() not in stop_words_en]\n",
        "    elif lang == 'fr':\n",
        "        return [token for token in tokens if token.lower() not in stop_words_fr]\n",
        "    elif lang == 'ar':\n",
        "        return [token for token in tokens if token not in stop_words_ar]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7htgxxKdgO8U"
      },
      "source": [
        "## Remove Rare Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIZ7W9XNgRrN"
      },
      "outputs": [],
      "source": [
        "def remove_rare_frequent_words(df):\n",
        "    # Count the word occurrences\n",
        "    word_counts = Counter(word for tweet in df['tweet'] for word in tweet.split())\n",
        "\n",
        "    # Identify rare and too frequent words\n",
        "    total_tweets = len(df)\n",
        "    rare_words = set(word for word, count in word_counts.items() if count <= 2) # Test This without rare words\n",
        "    frequent_words = set(word for word, count in word_counts.items() if count >= total_tweets * 0.95)\n",
        "    words_to_remove = rare_words | frequent_words\n",
        "\n",
        "    # Remove rare and too frequent words from tweets\n",
        "    df['tweet'] = df['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in words_to_remove]))\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk6a4aGojuU5"
      },
      "source": [
        "## Balance Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjonJk5ojw6a"
      },
      "outputs": [],
      "source": [
        "def balance_data(data):\n",
        "  return pd.concat([data[data['topic'] == topic].sample(n=data['topic'].value_counts().min(), random_state=42) for topic in data['topic'].unique()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkd90Nn1M5jI"
      },
      "source": [
        "## Clean DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLvmRc6iM4tq"
      },
      "outputs": [],
      "source": [
        "def clean_dataframe(data):\n",
        "    data['tweet'] = data['tweet'].str.lower()\n",
        "    data = data[(data['lang'].isin(['fr', 'ar', 'en']))]\n",
        "    data.drop_duplicates(subset=['tweet'],inplace=True)\n",
        "    data.dropna(subset=['tweet'],inplace=True)\n",
        "    data = data[data['tweet'] != '']\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl02nx8SndOu"
      },
      "source": [
        "#Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2idy6UGpH58"
      },
      "source": [
        "## Pipeline Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ktMydopJCs"
      },
      "outputs": [],
      "source": [
        "def data_processing_pipeline(data):\n",
        "\n",
        "    #Data Quality\n",
        "    data = data_quality(data)\n",
        "    #Clean DataFrame\n",
        "    data = clean_dataframe(data)\n",
        "    #Clean Tweets\n",
        "    data['tweet'] = data['tweet'].apply(clean_text)\n",
        "    #Tokenization\n",
        "    data['tweet'] = data.apply(lambda row: word_tokenization(row['tweet'], row['lang']), axis=1)\n",
        "    #Remove Stop Words\n",
        "    data['tweet'] = data.apply(lambda row: remove_stop_words(row['tweet'], row['lang']), axis=1)\n",
        "    # Join tokens back into a single string\n",
        "    data['tweet'] = data['tweet'].apply(lambda tokens: ' '.join(tokens))\n",
        "    #Clean DataFrame\n",
        "    data = clean_dataframe(data)\n",
        "    #Lemmatization\n",
        "    data['tweet'] = data.apply(lambda row: lemmatization(row['tweet'], row['lang']), axis=1)\n",
        "    #Reconstruct the tweet from lemmatized tokens\n",
        "    data['tweet'] = data['tweet'].apply(lambda tokens: ' '.join(tokens))\n",
        "    #Remove Rare and Frequent Words\n",
        "    data = remove_rare_frequent_words(data)\n",
        "    #Clean DataFrame\n",
        "    data = clean_dataframe(data)\n",
        "    #Balance Data\n",
        "    data = balance_data(data)\n",
        "\n",
        "    # Check if 'topic' column exists\n",
        "    if 'topic' in data.columns:\n",
        "        # Return 'topic' and 'tweet' columns\n",
        "        return data[['tweet','lang','topic']]\n",
        "    else:\n",
        "        # Return only 'tweet' column\n",
        "        return data['tweet']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaHSV0rU9uMT"
      },
      "source": [
        "## Execute The pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HURkgBc4tY60"
      },
      "outputs": [],
      "source": [
        "data_frame = load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXMKZMays_3K"
      },
      "outputs": [],
      "source": [
        "processed_data = data_processing_pipeline(data_frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9HJuHrQx8mS"
      },
      "outputs": [],
      "source": [
        "processed_data.to_csv('tweetsData.csv',index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "3Fq1vKXZn-ME",
        "0gybesKgpwwC",
        "qra6BEH6otis",
        "PlBDfOtwo0y3",
        "KinSC5Y4o-7l",
        "SPBWl9BApBJK",
        "Crvz6yp8loc4",
        "toPwlvoZoylr",
        "R3dXi13Fo3Iy",
        "Be3HVA07o5Ia",
        "rwsOsmC4o8lc",
        "7htgxxKdgO8U",
        "Fk6a4aGojuU5",
        "Xkd90Nn1M5jI",
        "N2idy6UGpH58",
        "DaHSV0rU9uMT"
      ],
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
